\section{Methodology\label{sec:methodology}}
We analyze the potential of FL for SMEs regarding the dimensions \emph{performance}, \emph{privacy}, and \emph{complexity} as introduced in section \ref{sec:literature_review}.

Depending on the research context, various perspectives can be taken regarding the analysis of these dimensions. However, these perspectives are not necessarily adequate for analyzing FL in the SME context. For example, in an application of FL for next-word prediction for mobile devices, one of the main considerations is that the number of samples per device (client) is magnitudes lower than the number of devices (clients) participating \citep{mcmahan2017communication}. This leads, e.g., to a very different perspective on complexity than it would be adequate in the SME context where the number of SMEs (clients) is relatively low and the number of samples per SME (client) is relatively high. Hence, for each dimension, we discuss which aspects of the dimension form the right perspective for assessing FL in the SME context. %In the dimension analysis we first discuss how to appropriately analyze each dimension regarding the specific needs, challenges and problems regarding machine learning decision systems for SMEs.

% For each dimension, after assessing the adequate perspective, we perform an analysis.
To evaluate the \emph{performance} dimension, we look at existing concepts in the literature and base our findings on a simulation pipeline we built that allows us to analyze realistic scenarios and hence, make results tangible. For \emph{privacy} and \emph{complexity}, we build on literature and qualitatively assess it.

Finally, we set the FL results in each dimension into perspective by comparing them to the \emph{one model per client} setting and the \emph{all data model} setting.
The \emph{one model per client} setting reflects the situation where each SME trains its own model solely on its own data. The \emph{all data model} reflects the hypothetical situation where one model is trained on the union of all SMEs' data. Although this setting is  mostly infeasible in practice because of privacy and security issues, it is suitable to be used as a reference point in the comparison. Hence, the settings could be seen as upper and lower bounds to the FL setting, depending on the analyzed dimension. This helps practitioners to interpret the results with regard to the two known alternatives.

For analyzing the performance dimension quantitatively, we implemented a flexible pipeline that allows us to simulate all three settings, the FL setting, the \emph{one model per client} setting, and the \emph{all data model} setting with various parameter combinations. We can, e.g., vary the number of clients (SMEs) taking part in the learning task, the data and label distribution, and several learning and evaluation-related parameters. The general ML problem we simulated is solving a binary classification task on a real-world industry image dataset. To do so, we used a convolutional neural network (CNN) being trained either for the \emph{one model per client} setting, the \emph{all data model} setting, or trained in a federated fashion using \emph{FederatedAveraging} (FedAvg) as first introduced by \citet{mcmahan2017communication}. We evaluate the performance using the AUC metric (area under the receiver operating characteristic (ROC) curve). In the following, we describe and motivate the choice of the dataset, the learning algorithm, the performance measure, and the study setup.

\subsection{Dataset\label{sec:methodology_dataset}} To realize the flexible simulation pipeline that allows for evaluating FL in a real-world context, our requirements for the dataset were the following: we sought a real-world industry dataset, publicly available, large enough to allow for the simulation of a reasonable number of clients and must have labels allowing supervised learning. Additionally, the supervised learning task should not be too challenging, keeping the computational complexity reasonable and making results available relatively quickly. It should, however, not be too simple, ensuring that performance improves with more data. Many publicly available, widely used, and industry-related datasets are either only suitable for unsupervised learning tasks or are not big enough. For example, the widely used industry-related dataset \emph{GC10-DET} by \citet{lv2020deep} only consists of 3.570 images containing ten classes. This relatively small number of images per class would restrict the choice of simulation scenarios and hence, the dataset did not meet our requirements. A dataset that fulfills all requirements is by \citet{schlagenhauf2021industrial} and was published in June 2021 at Karlsruhe Institute of Technology (KIT) in Germany. It consists of 21.853 images showing worn (= ``pitting'',  10.778 images) and unworn (= ``no pitting'', 11.075 images) parts of a spindle. Due to its size and simplicity, this dataset is particularly well suited to be used in our simulation pipeline.\footnote{We use an industry-related dataset instead of MNIST \cite{lecun2010mnist} or a dataset from LEAF \citep{caldas2018leaf} as we pursue to stay as close as possible to the SME and industry context.}

\subsection{Algorithm\label{sec:methodology_algorithm}}
We chose a convolutional neural network (CNN) to solve the binary image classification task on the chosen dataset \citep{schlagenhauf2021industrial} as CNNs are widely applied in image classification tasks \citep{lecun1995convolutional, krizhevsky2012imagenet}. To ensure the comparability between the results of the three settings (\emph{one model per client}, \emph{all data model}, and FL), we used the same network architecture, optimizer, and parameters for all settings. As a client optimizer in the FL setting and both other settings, we found the Adam optimizer \citep{adamkingma2014} with a learning rate of 0.001 in the keras \citep{keraschollet2015} implementation to work well in our simulation.\footnote{As noted in the TensorFlow Federated (TFF) documentation \citep{tensorflow2015-whitepaper}, we found ``a smaller learning rate than usual'' to perform superior to, e.g., the default learning rate of 0.01.} As server optimizer in the FL setting, we used the default combination of the stochastic gradient descent (SGD) optimizer with a learning rate of 1.0, as this recovers the FedAvg algorithm as noted in the TensorFlow Federated (TFF) documentation \citep{tensorflow2015-whitepaper}.
One possible drawback of using the same architecture for all settings is that the network architecture might not be optimal for each client. For clients with relatively little data, it might tend to overfit, and for clients with relatively large amounts of data, the performance might be restricted by the network's capacity. Still, the effect of this potential drawback might be neglectable. %is outweighed by the benefits that the comparability offers.
Due to the simplicity of the learning task, we assume that even for large clients, the capacity of the network is sufficient to meet the complexity of the data. At the same time, to prevent overfitting, we used an early stopping routine that stops the training if the validation loss of the model increases for two consecutive epochs. Additionally, optimizing hyperparameters for each setting and client would require infeasible amounts of computation time. % In practice, SMEs might adopt a network architecture that was found to perform well for the specific learning task if available.

FL is facilitated using the \emph{FederatedAveraging} algorithm \citep{mcmahan2017communication} in its TensorFlow Federated (TFF) \citep{tensorflow2015-whitepaper} implementation. In short, FedAvg works as follows: a central node coordinates the training. Models are trained locally on the client-side and after each epoch, the weights are sent to the central node. There, the weights are weighted by the number of training data and are sent back to the clients. This step of sending up and down model weights takes place until convergence or a certain stopping criterion is reached \citep{mcmahan2017communication}.

\subsection{Performance measure\label{sec:methodology_performance_measure}}
Our requirement for the performance measure is to enable comparability between and within clients. Often, metrics such as accuracy, precision, and recall (or related measures) are used. One example thereof is the mobile device setting \citep{mcmahan2017communication, yang2019federated}. However, these measures are not suited for comparing the performance across the settings and among the clients for two reasons. Firstly, each client's dataset could be strongly unbalanced regarding the label distribution, which would directly influence these metrics. For example, if one client has many ``pitting'' images, the trivial strategy of always predicting ``pitting'' would yield relatively high accuracy. Secondly, the CNN does not directly return the label ``pitting'' or ``no pitting'', but calculates a certainty score that can also be interpreted as the predicted probability of belonging to the ``pitting'' class. Hence, to calculate the accuracy, which needs the predicted classes as input and not score values, we need a threshold. Then, images with a score higher than the threshold are classified as ``pitting''. In practice, this score is a parameter that can be optimized by each individual company after model training has finished reflecting the respective costs of misclassification of this company. Consequently, we need a measure that abstracts from the unbalancedness of the dataset and that evaluates the score directly without the need for a threshold. One such metric often used in practice is the AUC (area under the ROC curve) \citep{hanley1982meaning}. AUC (on test data) is independent of changes in relative class proportions in the test data. Apart from the interpretation as the area under the ROC curve, the AUC can also be interpreted in the following way: If we draw pairs consisting of one ``pitting'' and one ``no pitting'' image, we let the classifier decide which of the two images is more likely to be the ``pitting'' image. The AUC can be calculated as the share of pairs, for which the correct image is classified as the ``pitting'' image. Hence, the AUC reports the "probability of correctly ranking a (normal, abnormal) pair" \citep{hanley1982meaning}. Thus, it fulfills our requirements and we chose it as our performance metric.

\subsection{Study setup\label{sec:methodology_study_setup}}
We use two settings to put the FL setting into perspective. First, the \emph{one model per client} setting where each client receives a certain (predefined) share of ``pitting'' and ``no pitting'' images of the overall dataset. After defining the shares for all clients, the allocation of samples takes place in a randomized way.
Second, the \emph{all data model} setting where the training takes place on the union of all clients' data, meaning all ``pitting'' images of the clients are combined to one big ``pitting'' dataset, same for the ``no pitting'' images.

We split the data into three sets: training data, validation data, and test data. Training took place on the training data. For applying the early stopping routine (see section \ref{sec:methodology_algorithm}), we used the validation data and for reporting an unbiased estimate of the final performance, we used the test data. For evaluation purposes, we implemented two ways of using the test set. Firstly, every client has its own test set in the \emph{one model per client} and the FL setting, which follows the same ``pitting'' and ``no pitting'' share as the whole client's dataset. One possible drawback of this individual test set per client is that clients with a relatively small amount of data have an even smaller amount of test data which could lead to instability in the results. But as our dataset with almost 22.000 images \citep{schlagenhauf2021industrial} is quite large, we counteract the problem by using a relatively high test share in general, so even for smaller clients, some stability in the results is given. A second drawback of this individual test set per client is that some clients can simply happen to have a relatively difficult test set, in the sense that it contains unproportionally many instances which are particularly hard to classify. Hence, their results are worse than for other clients. But this can simply happen in practice and underlines the real-world focus of our approach. Secondly, besides the individual test set per client, we implemented an alternative possibility to evaluate the results using a homogeneous test set for all clients and settings. Here, before any splitting and allocation of data takes place, a certain share of the dataset, e.g., 30 percent, is taken aside. This test set is used for evaluation after training. The major advantage of this approach is that every client model is evaluated on the same dataset, which yields more stability in the results. Although this approach provides an objective performance evaluation, it is not realistic in practice.
% We provide the results of the analyses using a unified test set in the appendix in section \ref{sec:unified_test_set_5_clients}.
As both approaches, the individual test set approach and the unified test set approach, come with advantages and drawbacks, we report and discuss both in this thesis.

To simulate real-world scenarios, we identified three parameters which we vary systematically. Firstly, the number of clients, secondly, the data distribution between the clients, and thirdly, the label distribution within each client. The data distribution indicates how the data is split among the clients. The amount of data for each client can either be balanced, meaning that each client has the same amount of data, or unbalanced, meaning that the amount of data varies among the clients. The label distribution indicates how the labels are distributed within each client. The label distribution can either be balanced or unbalanced, meaning that the amount of positive and negative examples per client is balanced or unbalanced, respectively. We call the combination of the second (data distribution) and third (label distribution) parameter the ``scenario''. For each scenario, we perform several ``simulations'' varying the number of participating clients. As the results of each simulation are stochastic due to, e.g., % the particular data that is allocated to the clients and
the parameter initialization, we made several ``runs'' for each simulation. We report the distribution of these results using boxplots and as a summary measure, we use the mean to represent the average or rather expected performance. Consequently, there are four scenarios (see also table \ref{tab:overview_scenarios}):
\begin{enumerate}
    \item \emph{balanced data distribution - balanced label distribution}: each client has the same amount of data and each client has as many ``pitting'' as ``no pitting'' images.
    \item \emph{unbalanced data distribution - balanced label distribution}: the amount of data varies among the clients and each client has as many ``pitting'' as ``no pitting'' images.
    \item \emph{balanced data distribution - unbalanced label distribution}: each client has the same amount of data and the clients have more or less ``pitting'' than ``no pitting'' images.
    \item \emph{unbalanced data distribution - unbalanced label distribution}: the amount of data varies among the clients and the clients have more or less ``pitting'' than ``no pitting'' images.
\end{enumerate}
\begin{table}[ht]
\centering
\caption{Overview of the four different scenarios}
\label{tab:overview_scenarios}
\begin{tabular}{lll} % {l|l|l}
\toprule
\begin{tabular}[c]{@{}l@{}}data distribution/\\ label distribution\end{tabular} & balanced   & unbalanced \\ \hline
balanced                                                                        & Scenario 1 & Scenario 2 \\ \hline
unbalanced                                                                      & Scenario 3 & Scenario 4 \\ %\hline
\bottomrule
\end{tabular}
\end{table}
The splitting routine between the clients regarding data and label distribution in our simulation pipeline takes place in a structured manner, dependent on the scenario. The data distribution and the label distribution for the balanced scenarios are trivial. An unbalanced data distribution means that half of the clients (or for an uneven number of clients, one client less than half) get four times more data than the other half of the clients. An unbalanced label distribution means that half of the clients (or for an uneven number of clients, one client more than half) get four times more ``pitting'' than ``no pitting'' images, the other half of the clients vice versa. We simulated both, balanced and unbalanced scenarios, in both data and label distribution, as these distributions or inequalities occur in the real-world industry. Hence, for each client, we defined how many ``pitting'' and ``no pitting'' images it receives. The data is then split randomly according to these rules.
